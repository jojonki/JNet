{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from process_data import save_pickle, load_pickle, load_processed_data, load_glove_weights, to_var, make_word_vector, add_padding\n",
    "from word_embedding import WordEmbedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from jnet import JNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.argv = ['a.py']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=8, help='input batch size')\n",
    "parser.add_argument('--embd_size', type=int, default=100, help='word embedding size')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 34762\n",
      "ctx_token_maxlen: 100\n",
      "query_token_maxlen: 40\n",
      "load ./pickles/glove_embd.pickle\n"
     ]
    }
   ],
   "source": [
    "# from process_data import save_pickle, load_pickle, load_task, load_glove_weights, to_var, make_word_vector, make_one_hot\n",
    "# train_data, train_ctx_maxlen = load_task('./dataset/train-v1.1.json')\n",
    "# save_pickle(train_data, './pickles/train_data.pickle')\n",
    "# train_data = load_pickle('./pickles/train_data.pickle')\n",
    "# dev_data, dev_ctx_maxlen = load_task('./dataset/dev-v1.1.json')\n",
    "# save_pickle(dev_data, './pickles/dev_data.pickle')\n",
    "# dev_data = load_pickle('./pickles/dev_data.pickle')\n",
    "# print('N train', len(train_data))\n",
    "# print('N dev', len(dev_data))\n",
    "# data = train_data+dev_data\n",
    "# ctx_maxlen = max(train_ctx_maxlen, dev_ctx_maxlen)\n",
    "# ctx_maxlen = 4063 #TODO\n",
    "# args.ctx_maxlen = ctx_maxlen\n",
    "# print('context char-level maxlen:', ctx_maxlen)\n",
    "train_data = load_processed_data('./dataset/train.txt')\n",
    "data = train_data\n",
    "vocab = set()\n",
    "for ctx, query, _ in data:\n",
    "    vocab |= set(ctx + query)\n",
    "    \n",
    "vocab = list(sorted(vocab)) + ['PAD']\n",
    "w2i = dict((w, i) for i, w in enumerate(vocab, 1))\n",
    "i2w = dict((i, w) for i, w in enumerate(vocab, 1))\n",
    "args.vocab_size = len(vocab)\n",
    "print('vocab size', len(vocab))\n",
    "\n",
    "ctx_token_maxlen = max([len(c) for c, _, _ in data])\n",
    "query_token_maxlen = max([len(q) for _, q, _ in data])\n",
    "print('ctx_token_maxlen:', ctx_token_maxlen)\n",
    "print('query_token_maxlen:', query_token_maxlen)\n",
    "args.ctx_token_maxlen = ctx_token_maxlen\n",
    "args.query_token_maxlen = query_token_maxlen\n",
    "args.answer_token_len = 1 # 2 TODO\n",
    "\n",
    "glove_embd = load_pickle('./pickles/glove_embd.pickle')\n",
    "# glove_embd = torch.from_numpy(load_glove_weights('./dataset', args.embd_size, len(vocab), w2i)).type(torch.FloatTensor)\n",
    "# save_pickle(glove_embd, './pickles/glove_embd.pickle')\n",
    "args.pre_embd = glove_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the',\n",
       "  'television',\n",
       "  'station',\n",
       "  ',',\n",
       "  'ndtv',\n",
       "  ',',\n",
       "  'grew',\n",
       "  'from',\n",
       "  'one',\n",
       "  'show',\n",
       "  'in',\n",
       "  '2002',\n",
       "  'to',\n",
       "  'a',\n",
       "  'full',\n",
       "  '24-hour',\n",
       "  'channel',\n",
       "  'with',\n",
       "  'original',\n",
       "  'programming',\n",
       "  'by',\n",
       "  'september',\n",
       "  '2006',\n",
       "  '.',\n",
       "  'wsnd-fm',\n",
       "  'serves',\n",
       "  'the',\n",
       "  'student',\n",
       "  'body',\n",
       "  'and',\n",
       "  'larger',\n",
       "  'south',\n",
       "  'bend',\n",
       "  'community',\n",
       "  'at',\n",
       "  '88.9',\n",
       "  'fm',\n",
       "  ',',\n",
       "  'offering',\n",
       "  'students',\n",
       "  'a',\n",
       "  'chance',\n",
       "  'to',\n",
       "  'become',\n",
       "  'involved',\n",
       "  'in',\n",
       "  'bringing',\n",
       "  'classical',\n",
       "  'music',\n",
       "  ',',\n",
       "  'fine',\n",
       "  'arts',\n",
       "  'and',\n",
       "  'educational',\n",
       "  'programming',\n",
       "  ',',\n",
       "  'and',\n",
       "  'alternative',\n",
       "  'rock',\n",
       "  'to',\n",
       "  'the',\n",
       "  'airwaves',\n",
       "  '.',\n",
       "  'another',\n",
       "  'radio',\n",
       "  'station',\n",
       "  ',',\n",
       "  'wvfi',\n",
       "  ',',\n",
       "  'began',\n",
       "  'as',\n",
       "  'a',\n",
       "  'partner',\n",
       "  'of',\n",
       "  'wsnd-fm',\n",
       "  '.',\n",
       "  'more',\n",
       "  'recently',\n",
       "  ',',\n",
       "  'however',\n",
       "  ',',\n",
       "  'wvfi',\n",
       "  'has',\n",
       "  'been',\n",
       "  'airing',\n",
       "  'independently',\n",
       "  'and',\n",
       "  'is',\n",
       "  'streamed',\n",
       "  'on',\n",
       "  'the',\n",
       "  'internet',\n",
       "  '.'],\n",
       " ['which',\n",
       "  'television',\n",
       "  'station',\n",
       "  'finds',\n",
       "  'its',\n",
       "  'home',\n",
       "  'at',\n",
       "  'notre',\n",
       "  'dame',\n",
       "  '?'],\n",
       " [4, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jnet import JNet\n",
    "def train(data, model, optimizer, loss_fn, n_epoch=5, batch_size=32):\n",
    "    for epoch in range(n_epoch):\n",
    "        print('---Epoch', epoch)\n",
    "        for i in tqdm(range(0, len(data)-batch_size, batch_size)): # TODO shuffle, last elms\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            c = [d[0] for d in batch_data]\n",
    "            q = [d[1] for d in batch_data]\n",
    "            context_var = make_word_vector(c, w2i, ctx_token_maxlen)\n",
    "            query_var = make_word_vector(q, w2i, query_token_maxlen)\n",
    "#             labels = [[d[4][0], d[5][0]] for d in batch_data]\n",
    "            labels = [d[2][0] for d in batch_data]\n",
    "            labels = to_var(torch.LongTensor(labels))\n",
    "            outs = model(context_var, query_var)\n",
    "            outs = outs.view(-1, ctx_token_maxlen) #(B*M, L)\n",
    "#             print('preds', torch.max(outs, 1)[1])\n",
    "            labels = labels.view(-1) # (B*M)\n",
    "#             print('labels', labels)\n",
    "            loss = loss_fn(outs, labels)\n",
    "            if i % (batch_size*10) == 0:\n",
    "                print(loss.data[0])\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             break\n",
    "        \n",
    "        _, preds = torch.max(outs, 1)\n",
    "        ct = 0\n",
    "        for pred, label in zip(preds, labels):\n",
    "            if pred.data[0] == label.data[0]: \n",
    "                ct+=1\n",
    "        print('Acc', ct, '/', len(preds))\n",
    "        break\n",
    "        \n",
    "    \n",
    "model = JNet(args)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# print(model)\n",
    "# for p in model.parameters():\n",
    "#     print(p)\n",
    "loss_fn = nn.NLLLoss()\n",
    "# optimizer = torch.optim.Adadelta(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=0.5, weight_decay=0.999)\n",
    "train(train_data, model, optimizer, loss_fn)\n",
    "print('fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_list(str_list):\n",
    "    return [str_var.lower() for str_var in str_list]\n",
    "def preprocess(fpath_read, fpath_write):\n",
    "    count = 0\n",
    "    fpr = open(fpath_read, 'r')\n",
    "    body = fpr.read()\n",
    "    js = json.loads(body)\n",
    "    fpw = open(fpath_write, 'w')\n",
    "    for c in js[\"data\"]:\n",
    "        print(c['title'])\n",
    "        for p in c[\"paragraphs\"]:\n",
    "            context = p[\"context\"].split(' ')\n",
    "            context_char = list(p[\"context\"])\n",
    "            context_pos = {}\n",
    "            for qa in p[\"qas\"]:\n",
    "\n",
    "                question = word_tokenize(qa[\"question\"])\n",
    "\n",
    "                for a in qa['answers']:\n",
    "                    answer = a['text'].strip()\n",
    "                    answer_start = int(a['answer_start'])\n",
    "\n",
    "                #add '.' here, just because NLTK is not good enough in some cases\n",
    "                answer_words = word_tokenize(answer+'.')\n",
    "                if answer_words[-1] == '.':\n",
    "                    answer_words = answer_words[:-1]\n",
    "                else:\n",
    "                    answer_words = lower_list(word_tokenize(answer))\n",
    "\n",
    "                prev_context_words = lower_list(word_tokenize( p[\"context\"][0:answer_start ] ))\n",
    "                left_context_words = lower_list(word_tokenize( p[\"context\"][answer_start:] ))\n",
    "                answer_reproduce = []\n",
    "                for i in range(len(answer_words)):\n",
    "                    if i < len(left_context_words):\n",
    "                        w = left_context_words[i]\n",
    "                        answer_reproduce.append(w)\n",
    "                join_a = ' '.join(answer_words)\n",
    "                join_ar = ' '.join(answer_reproduce)\n",
    "\n",
    "                #if not ((join_ar in join_a) or (join_a in join_ar)):\n",
    "                if join_a != join_ar:\n",
    "                    #print join_ar\n",
    "                    #print join_a\n",
    "                    #print 'answer:'+answer\n",
    "                    count += 1\n",
    "\n",
    "                fpw.write(' '.join(prev_context_words+left_context_words)+'\\t')\n",
    "                fpw.write(' '.join(question)+'\\t')\n",
    "                #fpw.write(join_a+'\\t')\n",
    "\n",
    "                pos_list = []\n",
    "                for i in range(len(answer_words)):\n",
    "                    if i < len(left_context_words):\n",
    "                        pos_list.append(str(len(prev_context_words)+i+1))\n",
    "                if len(pos_list) == 0:\n",
    "                    print (join_ar)\n",
    "                    print (join_a)\n",
    "                    print ('answer:'+answer)\n",
    "                assert(len(pos_list) > 0)\n",
    "                fpw.write(' '.join(pos_list)+'\\n')\n",
    "\n",
    "    fpw.close()\n",
    "print ('SQuAD preprossing finished!')\n",
    "preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a torch.Size([1, 2, 3])\n",
      "b torch.Size([1, 1, 3])\n",
      "\n",
      "(0 ,.,.) = \n",
      "  2  3  4\n",
      "  5  6  7\n",
      "[torch.FloatTensor of size 1x2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "]])\n",
    "print('a', a.size())\n",
    "b = torch.Tensor([[\n",
    "    [1, 1, 1]\n",
    "]])\n",
    "print('b', b.size())\n",
    "\n",
    "c = a+b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  0.2000  0.5000  0.3000\n",
      "  0.0000  0.0000  1.0000\n",
      "[torch.FloatTensor of size 1x2x3]\n",
      "\n",
      "\n",
      " 0.2000  0.5000  0.3000\n",
      " 0.0000  0.0000  1.0000\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  1  1  1\n",
      "  1  1  1\n",
      "[torch.FloatTensor of size 1x2x3]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.2894  0.3907  0.3199\n",
      "  0.2119  0.2119  0.5761\n",
      "[torch.FloatTensor of size 1x2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a=torch.Tensor([[\n",
    "    [0.2, 0.5, 0.3],\n",
    "    [0, 0, 1]\n",
    "]])\n",
    "print(a)\n",
    "print(a.view(-1, 3))\n",
    "print(F.softmax(a))\n",
    "print(F.softmax(a.view(-1, 3)).view(1, 2, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
